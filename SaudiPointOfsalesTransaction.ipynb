{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saudi Point Of sales Transaction dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "import pdfplumber\n",
    "import re\n",
    "from helper import *\n",
    "import datetime \n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from fake_useragent import UserAgent\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first page\n",
    "url = \"https://www.sama.gov.sa/ar-sa/Indices/pages/pos.aspx\"\n",
    "#second page \n",
    "#url= \"https://www.sama.gov.sa/ar-sa/Indices/Pages/POS.aspx?Paged=TRUE&p_SortBehavior=0&p_Created=20210316%2013%3a24%3a35&p_ID=50&PageFirstRow=31&View=e107b92a-9e94-4513-b248-dc9a0beeae39\"\n",
    "\n",
    "# sent off request object to a server to request and query some resource aas.\n",
    "with requests.Session() as s:\n",
    "\n",
    "    # The User-Agent request header is a characteristic string that lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent.\n",
    "    s.headers = {'User-Agent': 'Mozilla/5.0 '}\n",
    "\n",
    "    #Response object is generated once Requests gets a response back from the server. The Response object contains all of the information returned by the server and also contains the Request object we created originally\n",
    "    response = s.get(url)\n",
    "\n",
    "    #pulling data out of HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "\n",
    "    #The re.findall method scans string, searching for all matches and the pattern. It returns a list of strings in the matching order when scanning the string.\n",
    "    # re.MULTILINE tag affects where ^ and $ anchors match.\n",
    "    # re.DOTALL tag affects what the . pattern can match.\n",
    "    FileRef = re.findall(\"FileRef.*?[\\.!?]\", str(soup), re.MULTILINE | re.DOTALL )\n",
    "\n",
    "    # fix url using .replace method\n",
    "    FileRefNew = [item.replace(\"\\\\u002far-sa\\\\u002fIndices\\\\u002fPOS\\\\u002f\", \"https://www.sama.gov.sa/ar-sa/Indices/POS/\") for item in FileRef]\n",
    "    string = 'pdf'\n",
    "    #adding .pdf to the end of url\n",
    "    FileRefNew = [x + string for x in FileRefNew]\n",
    "    FileRefNew = [e[11:] for e in FileRefNew]\n",
    "    print(FileRefNew[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all urls in FileRefNew and download pdf file\n",
    "for url in FileRefNew:\n",
    "    try:\n",
    "        # Manage firefox specific settings in a way that geckodriver can understand \n",
    "        options = webdriver.FirefoxOptions()\n",
    "        options.set_preference(\"browser.download.folderList\", 2)\n",
    "        options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "        options.set_preference(\"browser.download.dir\", \"SaudiPointOfSales/pdf\")\n",
    "        options.set_preference(\"browser.download.useDownloadDir\", True)\n",
    "        options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf\")\n",
    "        options.set_preference(\"pdfjs.disabled\", True)\n",
    "        options.set_preference(\"pdfjs.enabledCache.state\", False)\n",
    "        options.set_preference(\"plugin.scan.Acrobat\", \"99.0\")\n",
    "        options.set_preference(\"plugin.scan.plid.all\", False)\n",
    "        # call selenium driver to automate web browser interaction\n",
    "        driver = webdriver.Firefox(executable_path=r\"SaudiPointOfSales/geckodriver\", options = options)\n",
    "        # open an URL\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(10)\n",
    "        driver.close()\n",
    "    except WebDriverException:\n",
    "        driver.quit()\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitch all pdf inside pdf folder using glob method \n",
    "arr_of_files = (glob.glob(\"./pdf/*.pdf\"))\n",
    "\n",
    "# Create or initialize Pandas DataFrame\n",
    "sectors_df = pd.DataFrame()\n",
    "cities_df = pd.DataFrame()\n",
    "\n",
    "# loop through each pdf and start scraping tables from pdfs  & save it in dataframs     \n",
    "for i in arr_of_files:\n",
    "    # Plumb a PDF for detailed information and table and store it in the initialize datafram.\n",
    "    with pdfplumber.open(i) as pdf:\n",
    "        tables = pdf.pages[0].find_tables()\n",
    "        first_table = tables[0].extract(x_tolerance = 5)\n",
    "        second_table = tables[1].extract(x_tolerance = 5)\n",
    "        \n",
    "        # Do some data cleansing in the first table inside a pdf \n",
    "        df_first_table = pd.DataFrame (first_table)\n",
    "        df_first_table = df_first_table[[0,7,8]]\n",
    "        df_first_table['Date'] = df_first_table[7][0]\n",
    "        df_first_table = df_first_table[2:]\n",
    "        df_first_table = df_first_table.reset_index(drop=True)\n",
    "        \n",
    "        sectors_df = sectors_df.append(df_first_table)\n",
    "        \n",
    "        # Do some data cleansing in the second table inside a pdf \n",
    "        df_second_table = pd.DataFrame (second_table)\n",
    "        df_second_table = df_second_table[[0,7,8]]\n",
    "        df_second_table['Date'] = df_second_table[7][0]\n",
    "        df_second_table = df_second_table[2:]\n",
    "        df_second_table = df_second_table.reset_index(drop=True)\n",
    "        \n",
    "        cities_df = cities_df.append(df_second_table)\n",
    "        \n",
    "print(\"** Done converting tables to data frames **\")\n",
    "\n",
    "# Renames columns  \n",
    "sectors_df = sectors_df.rename(columns={0: 'Sector',7: 'Number of Transactions', 8: 'Value of Transactions'})\n",
    "cities_df = cities_df.rename(columns={0: 'City',7: 'Number of Transactions', 8: 'Value of Transactions'})\n",
    "\n",
    "# change the columns value type from string to intger by using to_int function from helper \n",
    "to_int(cities_df,'Value of Transactions')\n",
    "to_int(cities_df,'Number of Transactions')\n",
    "\n",
    "\n",
    "\n",
    "# parsing dates\n",
    "#TODO: fix fix_date func\n",
    "# fix_date(sectors_df)\n",
    "# fix_date(cities_df)\n",
    "sectors_df[\"Start Date\"]= sectors_df['Date'].str.split(\"-\", n = 1, expand = True)[0]\n",
    "sectors_df['Start Date'] = [pd.to_datetime(x) for x in sectors_df['Start Date']]\n",
    "sectors_df[\"End Date\"]= sectors_df['Date'].str.split(\"-\", n = 1, expand = True)[1]\n",
    "sectors_df['End Date'] = [pd.to_datetime(x) for x in sectors_df['End Date']]\n",
    "sectors_df['Week Number'] = sectors_df['Start Date'].dt.week\n",
    "del sectors_df[\"Date\"]\n",
    "\n",
    "cities_df[\"Start Date\"]= cities_df['Date'].str.split(\"-\", n = 1, expand = True)[0]\n",
    "cities_df['Start Date'] = [pd.to_datetime(x) for x in cities_df['Start Date']]\n",
    "cities_df[\"End Date\"]= cities_df[\"Date\"].str.split(\"-\", n = 1, expand = True)[1]\n",
    "cities_df['End Date'] = [pd.to_datetime(x) for x in cities_df['End Date']]\n",
    "cities_df['Week Number'] = cities_df['Start Date'].dt.week\n",
    "del cities_df[\"Date\"]\n",
    "\n",
    "\n",
    "# Doing more data cleansing on city and sector columns\n",
    "sectors(sectors_df)\n",
    "cities(cities_df)\n",
    "\n",
    "\n",
    "# Add latitude & longitude for map chart using Nominatim\n",
    "group_City = cities_df.groupby(by='English_City').agg({'Value of Transactions' : 'sum', 'Number of Transactions' : 'sum'})\n",
    "group_City = group_City.reset_index()\n",
    "location = [x for x in group_City['English_City'].unique().tolist() \n",
    "            if type(x) == str]\n",
    "latitude = []\n",
    "longitude =  []\n",
    "for i in range(0, len(location)):\n",
    "    # remove things that does not seem usefull here\n",
    "    try:\n",
    "        address = location[i] + ', Saudi Arabia'\n",
    "        geolocator = Nominatim(user_agent=\"sa_explorer@gmail.com\")\n",
    "        loc = geolocator.geocode(address)\n",
    "        latitude.append(loc.latitude)\n",
    "        longitude.append(loc.longitude)\n",
    "        print('The geographical coordinate of location are {}, {}.'.format(loc.latitude, loc.longitude))\n",
    "    except:\n",
    "        # in the case the geolocator does not work, then add nan element to list\n",
    "        # to keep the right size\n",
    "        latitude.append(np.nan)\n",
    "        longitude.append(np.nan)\n",
    "# create a dataframe with the locatio, latitude and longitude\n",
    "df_ = pd.DataFrame({'English_City':location, \n",
    "                    'location_latitude': latitude,\n",
    "                    'location_longitude':longitude})\n",
    "# merge on English_City with Groupe_City to get the column \n",
    "Grouped_City = group_City.merge(df_, on='English_City', how='left')\n",
    "Grouped_City.at[Grouped_City['English_City'] == 'OTHER','location_latitude'] = float(25)\n",
    "Grouped_City.at[Grouped_City['English_City'] == 'OTHER','location_longitude'] = float(45)\n",
    "\n",
    "# change the order of dfs columns & export it as csv\n",
    "columnsTitles = ['English_Sector', 'Arabic_Sector','Start Date','End Date','Week Number', 'Number of Transactions', 'Value of Transactions']\n",
    "sectors_df = sectors_df.reindex(columns=columnsTitles)\n",
    "sectors_df.to_csv('output/sectors_df.csv', index = False)\n",
    "columnsTitles = ['English_City', 'Arabic_City','Start Date','End Date','Week Number', 'Number of Transactions', 'Value of Transactions']\n",
    "cities_df = cities_df.reindex(columns=columnsTitles)\n",
    "cities_df.to_csv('output/cities_df.csv', index = False)\n",
    "Grouped_City.to_csv('output/Grouped_City.csv', index = False)\n",
    "# merge cities_df and Grouped_City in order to standerlize df, it we help us alot once we plot our data\n",
    "full_cities_df = pd.merge(cities_df, Grouped_City[['English_City', 'location_latitude', 'location_longitude']], on='English_City')\n",
    "full_cities_df.to_csv('output/full_cities_df.csv', index = False)\n",
    "\n",
    "print(\"** Done cleansing data frames **\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
