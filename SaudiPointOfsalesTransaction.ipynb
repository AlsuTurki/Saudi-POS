{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saudi Point Of sales Transaction dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "import pdfplumber\n",
    "import re\n",
    "from dfhelper import *\n",
    "from datetime import datetime\n",
    "from dateutil import parser \n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from fake_useragent import UserAgent\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly pos transaction\n",
    "#first page\n",
    "url = \"https://www.sama.gov.sa/ar-sa/EconomicReports/pages/monthlystatistics.aspx\"\n",
    "\n",
    "# sent off request object to a server to request and query some resource aas.\n",
    "with requests.Session() as s:\n",
    "\n",
    "    # The User-Agent request header is a characteristic string that lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent.\n",
    "    s.headers = {'User-Agent': 'Mozilla/5.0 '}\n",
    "\n",
    "    #Response object is generated once Requests gets a response back from the server. The Response object contains all of the information returned by the server and also contains the Request object we created originally\n",
    "    response = s.get(url)\n",
    "\n",
    "    #pulling data out of HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    #FileRef = re.findall(\"xlsx\", str(soup), re.MULTILINE | re.DOTALL )\n",
    "\n",
    "    FileRef = re.findall(\"FileRef.*?[\\.!?]\", str(soup), re.MULTILINE | re.DOTALL )\n",
    "\n",
    "    #fix url using .replace method\n",
    "    FileRefNew = [item.replace(\"\\\\u002far-sa\\\\u002fEconomicReports\\\\u002fMonthlyStatistics\\\\u002f\", \"https://www.sama.gov.sa/ar-sa/EconomicReports/MonthlyStatistics/\") for item in FileRef]\n",
    "    string = 'xlsx'\n",
    "    #adding .pdf to the end of url\n",
    "    FileRefNew = [x + string for x in FileRefNew]\n",
    "    FileRefNew = [e[11:] for e in FileRefNew]\n",
    "    print(FileRefNew[0])\n",
    "\n",
    "\n",
    "try:\n",
    "    # Manage firefox specific settings in a way that geckodriver can understand \n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)\n",
    "    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    options.set_preference(\"browser.download.dir\", \"./input\")\n",
    "    options.set_preference(\"browser.download.useDownloadDir\", True)\n",
    "    #options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/xls;text/csv\")\n",
    "    options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "    # call selenium driver to automate web browser interaction\n",
    "    driver = webdriver.Firefox(executable_path=\"./geckodriver\", options = options)\n",
    "    # open an URL\n",
    "    driver.get(FileRefNew[0])\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.close()\n",
    "except WebDriverException:\n",
    "    driver.quit()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly pos transaction\n",
    "#first page\n",
    "url = \"https://www.sama.gov.sa/ar-sa/Indices/pages/pos.aspx\"\n",
    "\n",
    "# sent off request object to a server to request and query some resource aas.\n",
    "with requests.Session() as s:\n",
    "\n",
    "    # The User-Agent request header is a characteristic string that lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent.\n",
    "    s.headers = {'User-Agent': 'Mozilla/5.0 '}\n",
    "\n",
    "    #Response object is generated once Requests gets a response back from the server. The Response object contains all of the information returned by the server and also contains the Request object we created originally\n",
    "    response = s.get(url)\n",
    "\n",
    "    #pulling data out of HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "\n",
    "    #The re.findall method scans string, searching for all matches and the pattern. It returns a list of strings in the matching order when scanning the string.\n",
    "    # re.MULTILINE tag affects where ^ and $ anchors match.\n",
    "    # re.DOTALL tag affects what the . pattern can match.\n",
    "    FileRef = re.findall(\"FileRef.*?[\\.!?]\", str(soup), re.MULTILINE | re.DOTALL )\n",
    "\n",
    "    # fix url using .replace method\n",
    "    FileRefNew = [item.replace(\"\\\\u002far-sa\\\\u002fIndices\\\\u002fPOS\\\\u002f\", \"https://www.sama.gov.sa/ar-sa/Indices/POS/\") for item in FileRef]\n",
    "    string = 'pdf'\n",
    "    #adding .pdf to the end of url\n",
    "    FileRefNew = [x + string for x in FileRefNew]\n",
    "    FileRefNew = [e[11:] for e in FileRefNew]\n",
    "    print(FileRefNew[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all urls in FileRefNew and download pdf file\n",
    "for url in FileRefNew:\n",
    "    try:\n",
    "        # Manage firefox specific settings in a way that geckodriver can understand \n",
    "        options = webdriver.FirefoxOptions()\n",
    "        options.set_preference(\"browser.download.folderList\", 2)\n",
    "        options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "        options.set_preference(\"browser.download.dir\", \"./pdf\")\n",
    "        options.set_preference(\"browser.download.useDownloadDir\", True)\n",
    "        options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf\")\n",
    "        options.set_preference(\"pdfjs.disabled\", True)\n",
    "        options.set_preference(\"pdfjs.enabledCache.state\", False)\n",
    "        options.set_preference(\"plugin.scan.Acrobat\", \"99.0\")\n",
    "        options.set_preference(\"plugin.scan.plid.all\", False)\n",
    "        # call selenium driver to automate web browser interaction\n",
    "        driver = webdriver.Firefox(executable_path=\"./geckodriver\", options = options)\n",
    "        # open an URL\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(10)\n",
    "        driver.close()\n",
    "    except WebDriverException:\n",
    "        driver.quit()\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitch all pdf inside pdf folder using glob method \n",
    "arr_of_files = (glob.glob(\"./pdf/*.pdf\"))\n",
    "\n",
    "# Create or initialize Pandas DataFrame\n",
    "sectors_df = pd.DataFrame()\n",
    "cities_df = pd.DataFrame()\n",
    "\n",
    "# loop through each pdf and start scraping tables from pdfs  & save it in dataframs     \n",
    "for i in arr_of_files:\n",
    "    # Plumb a PDF for detailed information and table and store it in the initialize datafram.\n",
    "    with pdfplumber.open(i) as pdf:\n",
    "        tables = pdf.pages[0].find_tables()\n",
    "        first_table = tables[0].extract(x_tolerance = 5)\n",
    "        if '2020' in i:\n",
    "            second_table = tables[2].extract(x_tolerance = 5)\n",
    "        else:\n",
    "            second_table = tables[1].extract(x_tolerance = 5)\n",
    "        \n",
    "        # Do some data cleansing in the first table inside a pdf \n",
    "        df_first_table = pd.DataFrame (first_table)\n",
    "        df_first_table = df_first_table[[0,7,8]]\n",
    "        df_first_table['Date'] = df_first_table[7][0]\n",
    "        df_first_table = df_first_table[2:]\n",
    "        df_first_table = df_first_table.reset_index(drop=True)\n",
    "        \n",
    "        sectors_df = sectors_df.append(df_first_table)\n",
    "        \n",
    "        # Do some data cleansing in the second table inside a pdf \n",
    "        df_second_table = pd.DataFrame (second_table)\n",
    "        df_second_table = df_second_table[[0,7,8]]\n",
    "        df_second_table['Date'] = df_second_table[7][0]\n",
    "        df_second_table = df_second_table[2:]\n",
    "        df_second_table = df_second_table.reset_index(drop=True)\n",
    "        cities_df = cities_df.append(df_second_table)\n",
    "        \n",
    "print(\"** Done converting tables to data frames **\")\n",
    "\n",
    "# Renames columns  \n",
    "sectors_df = sectors_df.rename(columns={0: 'Sector',7: 'Number of Transactions', 8: 'Value of Transactions'})\n",
    "cities_df = cities_df.rename(columns={0: 'City',7: 'Number of Transactions', 8: 'Value of Transactions'})\n",
    "cities_df = cities_df.drop(cities_df[cities_df['Value of Transactions']=='Value of \\nTransactions'].index)\n",
    "# change the columns value type from string to intger by using to_int function from helper\n",
    "sectors_df = sectors_df.dropna(how='any',axis=0)\n",
    "to_float(cities_df,'Value of Transactions')\n",
    "to_int(cities_df,'Number of Transactions')\n",
    "to_float(sectors_df,'Value of Transactions')\n",
    "to_int(sectors_df,'Number of Transactions')\n",
    "\n",
    "# parsing dates\n",
    "\n",
    "sectors_df[\"End Date\"]= sectors_df['Date'].str.split(\"-\", n = 1, expand = True)[1]\n",
    "# if statment due to missing year in 2020 pdfs \n",
    "if ',21' in sectors_df[\"End Date\"].values:\n",
    "    pass\n",
    "else:\n",
    "    sectors_df[\"End Date\"]= sectors_df[\"End Date\"] + ',20'\n",
    "sectors_df['End Date'] = [pd.to_datetime(x) for x in sectors_df['End Date']]\n",
    "sectors_df['Date'] = sectors_df['End Date']\n",
    "del sectors_df['End Date']\n",
    "\n",
    "\n",
    "cities_df[\"End Date\"]= cities_df['Date'].str.split(\"-\", n = 1, expand = True)[1]\n",
    "# if statment due to missing year in 2020 pdfs \n",
    "if ',21' in cities_df[\"End Date\"].values:\n",
    "    pass\n",
    "else:\n",
    "    cities_df[\"End Date\"]= cities_df[\"End Date\"] + ',20'\n",
    "cities_df['End Date'] = [pd.to_datetime(x) for x in cities_df['End Date']]\n",
    "cities_df['Date'] = cities_df['End Date']\n",
    "del cities_df['End Date']\n",
    "\n",
    "# Doing more data cleansing on city and sector columns\n",
    "sectors(sectors_df)\n",
    "cities(cities_df)\n",
    "\n",
    "\n",
    "# Add latitude & longitude for map chart using Nominatim\n",
    "group_City = cities_df.groupby(by='English_City').agg({'Value of Transactions' : 'sum', 'Number of Transactions' : 'sum'})\n",
    "group_City = group_City.reset_index()\n",
    "location = [x for x in group_City['English_City'].unique().tolist() \n",
    "            if type(x) == str]\n",
    "latitude = []\n",
    "longitude =  []\n",
    "for i in range(0, len(location)):\n",
    "    # remove things that does not seem usefull here\n",
    "    try:\n",
    "        address = location[i] + ', Saudi Arabia'\n",
    "        geolocator = Nominatim(user_agent=\"sa_explorer@gmail.com\")\n",
    "        loc = geolocator.geocode(address)\n",
    "        latitude.append(loc.latitude)\n",
    "        longitude.append(loc.longitude)\n",
    "        print('The geographical coordinate of location are {}, {}.'.format(loc.latitude, loc.longitude))\n",
    "    except:\n",
    "        # in the case the geolocator does not work, then add nan element to list\n",
    "        # to keep the right size\n",
    "        latitude.append(np.nan)\n",
    "        longitude.append(np.nan)\n",
    "# create a dataframe with the locatio, latitude and longitude\n",
    "df_ = pd.DataFrame({'English_City':location, \n",
    "                    'location_latitude': latitude,\n",
    "                    'location_longitude':longitude})\n",
    "# merge on English_City with Groupe_City to get the column \n",
    "Grouped_City = group_City.merge(df_, on='English_City', how='left')\n",
    "Grouped_City.at[Grouped_City['English_City'] == 'OTHER','location_latitude'] = float(25)\n",
    "Grouped_City.at[Grouped_City['English_City'] == 'OTHER','location_longitude'] = float(45)\n",
    "\n",
    "# change the order of dfs columns & export it as csv\n",
    "columnsTitles = ['English_Sector', 'Arabic_Sector','Date', 'Number of Transactions', 'Value of Transactions']\n",
    "sectors_df = sectors_df.reindex(columns=columnsTitles)\n",
    "columnsTitles = ['English_City', 'Arabic_City','Date', 'Number of Transactions', 'Value of Transactions']\n",
    "cities_df = cities_df.reindex(columns=columnsTitles)\n",
    "\n",
    "sectors_df.to_csv('output/sectors_df.csv', index = False)\n",
    "# merge cities_df and Grouped_City in order to standerlize df, it we help us alot once we plot our data\n",
    "full_cities_df = pd.merge(cities_df, Grouped_City[['English_City', 'location_latitude', 'location_longitude']], on='English_City')\n",
    "full_cities_df.to_csv('output/full_cities_df.csv', index = False)\n",
    "\n",
    "print(\"** Done cleansing data frames **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly POS By Sector \n",
    "df = pd.read_excel(\"./input/Monthly_Bulletin_September2021.xlsx\",\n",
    "                   sheet_name='30d', engine='openpyxl', skiprows=14)\n",
    "columns_name = {'الفترة': 'Date'}\n",
    "df.rename(columns=columns_name, inplace=True)\n",
    "df.drop(df.columns[[0,2, 18, 35]], axis=1, inplace=True)\n",
    "df.dropna(axis = 0, how = 'all', inplace = True)\n",
    "df.dropna(axis = 1, how = 'all', inplace = True)\n",
    "df.drop(df.tail(4).index, inplace = True)\n",
    "df.drop(df.index[[0,1,2]], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.drop(df.index[0:27], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "#by anonymouse hero  \n",
    "columnsTitles = ['Date', 'Number of Transactions', 'Value of Transactions']\n",
    "Sector_final = pd.DataFrame(columns=columnsTitles)\n",
    "sector_list_en = ['Transportation','Health','Restaurants & Café','Hotels','Beverage and Food','Clothing and Footwear',\n",
    "                'Recreation and Culture','Miscellaneous Goods and Services','Electronic & Electric Devices ','Furniture',\n",
    "                'Construction & Building Materials','Jewelry','Telecommunication','Education','Public Utilities','Others']\n",
    "sector_list_ar = ['المواصلات','الصحة','المطاعم والمقاهي','الفنادق','المشروبات والأطعمة','الملابس والأحذية','الترفيه والثقافة','خدمات وسلع متنوعة*','الأجهزه الإلكترونية والكهربائية','الأثاث','مواد البناء و التعمير','المجوهرات ','الاتصالات','التعليم','المنافع العامة','أخرى']\n",
    "x = 1\n",
    "y = 2\n",
    "\n",
    "for i, j in zip(sector_list_en, sector_list_ar):\n",
    "    initial = df.iloc[:, [0, x,y]]\n",
    "\n",
    "    mapping = {initial.columns[1]: 'Number of Transactions',\n",
    "               initial.columns[2]: 'Value of Transactions'}\n",
    "    initial = initial.rename(columns=mapping)\n",
    "\n",
    "    initial['English_Sector'] = i\n",
    "    initial['Arabic_Sector'] = j\n",
    "    x = x + 2\n",
    "    y = y + 2\n",
    "    Sector_final = Sector_final.append(initial)\n",
    "Sector_final.to_csv(\"./output/Monthly_pos_by_Sector.csv\", index = False)\n",
    "\n",
    "# Monthly POS By City \n",
    "df = pd.read_excel(\"./input/Monthly_Bulletin_September2021.xlsx\",\n",
    "                   sheet_name='30e', engine='openpyxl', skiprows=14)\n",
    "df.dropna(axis = 0, how = 'all', inplace = True)\n",
    "df.dropna(axis = 1, how = 'all', inplace = True)\n",
    "df.drop(df.index[[0]], inplace=True)\n",
    "df.drop(df.index[2:30], inplace=True)\n",
    "df.dropna(axis = 1, how = 'all', inplace = True)\n",
    "df.drop(df.columns[[16, 32]], axis=1, inplace=True)\n",
    "columns_name = {'الفترة': 'Date'}\n",
    "df.rename(columns=columns_name, inplace=True)\n",
    "df.drop(df.index[[0, 1]], inplace=True)\n",
    "#by anonymouse hero  \n",
    "columnsTitles = ['Date', 'Number of Transactions', 'Value of Transactions','Number of Terminals']\n",
    "City_final = pd.DataFrame(columns=columnsTitles)\n",
    "city_list_en = ['Riyadh', 'Jeddah','Dammam', 'AL-Madinah','Makkah','Buraidah','Tabuk','Hail',\n",
    "             'Abha','Jazan','Najran','Skaka','Arar','AL-Bahah']\n",
    "city_list_ar = ['الرياض', 'جدة','الدمام', 'المدينة المنورة','مكة المكرمة','بريدة','تبوك','حائل',\n",
    "             'أبها','جازان','نجران','سكاكا','عرعر','الباحة']\n",
    "x = 1\n",
    "y = 2\n",
    "z = 3\n",
    "\n",
    "for i, j in zip(city_list_en, city_list_ar):\n",
    "    initial = df.iloc[:, [0, x,y,z]]\n",
    "\n",
    "    mapping = {initial.columns[1]: 'Number of Transactions',\n",
    "               initial.columns[2]: 'Value of Transactions', \n",
    "               initial.columns[3]: 'Number of Terminals' }\n",
    "    initial = initial.rename(columns=mapping)\n",
    "\n",
    "    initial['English_city'] = i\n",
    "    initial['Arabic_City'] = j\n",
    "    x = x + 3\n",
    "    y = y + 3\n",
    "    z = z + 3\n",
    "    City_final = City_final.append(initial)\n",
    "\n",
    "\n",
    "# Add latitude & longitude for map chart using Nominatim\n",
    "group_City = City_final.groupby(by='English_city').agg({'Value of Transactions' : 'sum', 'Number of Transactions' : 'sum'})\n",
    "group_City = group_City.reset_index()\n",
    "location = [x for x in group_City['English_city'].unique().tolist() \n",
    "            if type(x) == str]\n",
    "latitude = []\n",
    "longitude =  []\n",
    "for i in range(0, len(location)):\n",
    "    # remove things that does not seem usefull here\n",
    "    try:\n",
    "        address = location[i] + ', Saudi Arabia'\n",
    "        geolocator = Nominatim(user_agent=\"sa_explorer@gmail.com\")\n",
    "        loc = geolocator.geocode(address)\n",
    "        latitude.append(loc.latitude)\n",
    "        longitude.append(loc.longitude)\n",
    "    except:\n",
    "        # in the case the geolocator does not work, then add nan element to list\n",
    "        # to keep the right size\n",
    "        latitude.append(np.nan)\n",
    "        longitude.append(np.nan)\n",
    "# create a dataframe with the locatio, latitude and longitude\n",
    "df_ = pd.DataFrame({'English_city':location, \n",
    "                    'location_latitude': latitude,\n",
    "                    'location_longitude':longitude})\n",
    "# merge on English_City with Groupe_City to get the column \n",
    "Grouped_City = group_City.merge(df_, on='English_city', how='left')\n",
    "City_final = pd.merge(City_final, Grouped_City[['English_city', 'location_latitude', 'location_longitude']], on='English_city')\n",
    "City_final.to_csv(\"./output/Monthly_pos_by_citites.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
