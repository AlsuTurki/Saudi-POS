{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saudi Point Of sales Transaction dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "import pdfplumber\n",
    "import re\n",
    "from dfhelper import *\n",
    "from datetime import datetime\n",
    "from dateutil import parser \n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from fake_useragent import UserAgent\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first page\n",
    "url = \"https://www.sama.gov.sa/ar-sa/Indices/pages/pos.aspx\"\n",
    "#second page \n",
    "#url= \"https://www.sama.gov.sa/ar-sa/Indices/Pages/POS.aspx?Paged=TRUE&p_SortBehavior=0&p_Created=20210316%2013%3a24%3a35&p_ID=50&PageFirstRow=31&View=e107b92a-9e94-4513-b248-dc9a0beeae39\"\n",
    "#third page \n",
    "#url = \"https://www.sama.gov.sa/ar-sa/Indices/Pages/POS.aspx?Paged=TRUE&p_SortBehavior=0&p_Created=20200825%2011%3a47%3a36&p_ID=18&PageFirstRow=61&View=e107b92a-9e94-4513-b248-dc9a0beeae39\"\n",
    "\n",
    "# sent off request object to a server to request and query some resource aas.\n",
    "with requests.Session() as s:\n",
    "\n",
    "    # The User-Agent request header is a characteristic string that lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent.\n",
    "    s.headers = {'User-Agent': 'Mozilla/5.0 '}\n",
    "\n",
    "    #Response object is generated once Requests gets a response back from the server. The Response object contains all of the information returned by the server and also contains the Request object we created originally\n",
    "    response = s.get(url)\n",
    "\n",
    "    #pulling data out of HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "\n",
    "    #The re.findall method scans string, searching for all matches and the pattern. It returns a list of strings in the matching order when scanning the string.\n",
    "    # re.MULTILINE tag affects where ^ and $ anchors match.\n",
    "    # re.DOTALL tag affects what the . pattern can match.\n",
    "    FileRef = re.findall(\"FileRef.*?[\\.!?]\", str(soup), re.MULTILINE | re.DOTALL )\n",
    "\n",
    "    # fix url using .replace method\n",
    "    FileRefNew = [item.replace(\"\\\\u002far-sa\\\\u002fIndices\\\\u002fPOS\\\\u002f\", \"https://www.sama.gov.sa/ar-sa/Indices/POS/\") for item in FileRef]\n",
    "    string = 'pdf'\n",
    "    #adding .pdf to the end of url\n",
    "    FileRefNew = [x + string for x in FileRefNew]\n",
    "    FileRefNew = [e[11:] for e in FileRefNew]\n",
    "    print(FileRefNew[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all urls in FileRefNew and download pdf file\n",
    "for url in FileRefNew:\n",
    "    try:\n",
    "        # Manage firefox specific settings in a way that geckodriver can understand \n",
    "        options = webdriver.FirefoxOptions()\n",
    "        options.set_preference(\"browser.download.folderList\", 2)\n",
    "        options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "        options.set_preference(\"browser.download.dir\", \"./pdf\")\n",
    "        options.set_preference(\"browser.download.useDownloadDir\", True)\n",
    "        options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf\")\n",
    "        options.set_preference(\"pdfjs.disabled\", True)\n",
    "        options.set_preference(\"pdfjs.enabledCache.state\", False)\n",
    "        options.set_preference(\"plugin.scan.Acrobat\", \"99.0\")\n",
    "        options.set_preference(\"plugin.scan.plid.all\", False)\n",
    "        # call selenium driver to automate web browser interaction\n",
    "        driver = webdriver.Firefox(executable_path=\"./geckodriver\", options = options)\n",
    "        # open an URL\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(10)\n",
    "        driver.close()\n",
    "    except WebDriverException:\n",
    "        driver.quit()\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitch all pdf inside pdf folder using glob method \n",
    "arr_of_files = (glob.glob(\"./pdf/*.pdf\"))\n",
    "\n",
    "# Create or initialize Pandas DataFrame\n",
    "sectors_df = pd.DataFrame()\n",
    "cities_df = pd.DataFrame()\n",
    "\n",
    "# loop through each pdf and start scraping tables from pdfs  & save it in dataframs     \n",
    "for i in arr_of_files:\n",
    "    # Plumb a PDF for detailed information and table and store it in the initialize datafram.\n",
    "    with pdfplumber.open(i) as pdf:\n",
    "        tables = pdf.pages[0].find_tables()\n",
    "        first_table = tables[0].extract(x_tolerance = 5)\n",
    "        if '2020' in i:\n",
    "            second_table = tables[2].extract(x_tolerance = 5)\n",
    "        else:\n",
    "            second_table = tables[1].extract(x_tolerance = 5)\n",
    "        \n",
    "        # Do some data cleansing in the first table inside a pdf \n",
    "        df_first_table = pd.DataFrame (first_table)\n",
    "        df_first_table = df_first_table[[0,7,8]]\n",
    "        df_first_table['Date'] = df_first_table[7][0]\n",
    "        df_first_table = df_first_table[2:]\n",
    "        df_first_table = df_first_table.reset_index(drop=True)\n",
    "        \n",
    "        sectors_df = sectors_df.append(df_first_table)\n",
    "        \n",
    "        # Do some data cleansing in the second table inside a pdf \n",
    "        df_second_table = pd.DataFrame (second_table)\n",
    "        df_second_table = df_second_table[[0,7,8]]\n",
    "        df_second_table['Date'] = df_second_table[7][0]\n",
    "        df_second_table = df_second_table[2:]\n",
    "        df_second_table = df_second_table.reset_index(drop=True)\n",
    "        cities_df = cities_df.append(df_second_table)\n",
    "        \n",
    "print(\"** Done converting tables to data frames **\")\n",
    "\n",
    "# Renames columns  \n",
    "sectors_df = sectors_df.rename(columns={0: 'Sector',7: 'Number of Transactions', 8: 'Value of Transactions'})\n",
    "cities_df = cities_df.rename(columns={0: 'City',7: 'Number of Transactions', 8: 'Value of Transactions'})\n",
    "cities_df = cities_df.drop(cities_df[cities_df['Value of Transactions']=='Value of \\nTransactions'].index)\n",
    "# change the columns value type from string to intger by using to_int function from helper\n",
    "sectors_df = sectors_df.dropna(how='any',axis=0)\n",
    "to_float(cities_df,'Value of Transactions')\n",
    "to_int(cities_df,'Number of Transactions')\n",
    "to_float(sectors_df,'Value of Transactions')\n",
    "to_int(sectors_df,'Number of Transactions')\n",
    "\n",
    "# parsing dates\n",
    "\n",
    "sectors_df[\"End Date\"]= sectors_df['Date'].str.split(\"-\", n = 1, expand = True)[1]\n",
    "# if statment due to missing year in 2020 pdfs \n",
    "if ',21' in sectors_df[\"End Date\"].values:\n",
    "    pass\n",
    "else:\n",
    "    sectors_df[\"End Date\"]= sectors_df[\"End Date\"] + ',20'\n",
    "sectors_df['End Date'] = [pd.to_datetime(x) for x in sectors_df['End Date']]\n",
    "sectors_df['Date'] = sectors_df['End Date']\n",
    "del sectors_df['End Date']\n",
    "\n",
    "\n",
    "cities_df[\"End Date\"]= cities_df['Date'].str.split(\"-\", n = 1, expand = True)[1]\n",
    "# if statment due to missing year in 2020 pdfs \n",
    "if ',21' in cities_df[\"End Date\"].values:\n",
    "    pass\n",
    "else:\n",
    "    cities_df[\"End Date\"]= cities_df[\"End Date\"] + ',20'\n",
    "cities_df['End Date'] = [pd.to_datetime(x) for x in cities_df['End Date']]\n",
    "cities_df['Date'] = cities_df['End Date']\n",
    "del cities_df['End Date']\n",
    "\n",
    "# Doing more data cleansing on city and sector columns\n",
    "sectors(sectors_df)\n",
    "cities(cities_df)\n",
    "\n",
    "\n",
    "# Add latitude & longitude for map chart using Nominatim\n",
    "group_City = cities_df.groupby(by='English_City').agg({'Value of Transactions' : 'sum', 'Number of Transactions' : 'sum'})\n",
    "group_City = group_City.reset_index()\n",
    "location = [x for x in group_City['English_City'].unique().tolist() \n",
    "            if type(x) == str]\n",
    "latitude = []\n",
    "longitude =  []\n",
    "for i in range(0, len(location)):\n",
    "    # remove things that does not seem usefull here\n",
    "    try:\n",
    "        address = location[i] + ', Saudi Arabia'\n",
    "        geolocator = Nominatim(user_agent=\"sa_explorer@gmail.com\")\n",
    "        loc = geolocator.geocode(address)\n",
    "        latitude.append(loc.latitude)\n",
    "        longitude.append(loc.longitude)\n",
    "        print('The geographical coordinate of location are {}, {}.'.format(loc.latitude, loc.longitude))\n",
    "    except:\n",
    "        # in the case the geolocator does not work, then add nan element to list\n",
    "        # to keep the right size\n",
    "        latitude.append(np.nan)\n",
    "        longitude.append(np.nan)\n",
    "# create a dataframe with the locatio, latitude and longitude\n",
    "df_ = pd.DataFrame({'English_City':location, \n",
    "                    'location_latitude': latitude,\n",
    "                    'location_longitude':longitude})\n",
    "# merge on English_City with Groupe_City to get the column \n",
    "Grouped_City = group_City.merge(df_, on='English_City', how='left')\n",
    "Grouped_City.at[Grouped_City['English_City'] == 'OTHER','location_latitude'] = float(25)\n",
    "Grouped_City.at[Grouped_City['English_City'] == 'OTHER','location_longitude'] = float(45)\n",
    "\n",
    "# change the order of dfs columns & export it as csv\n",
    "columnsTitles = ['English_Sector', 'Arabic_Sector','Date', 'Number of Transactions', 'Value of Transactions']\n",
    "sectors_df = sectors_df.reindex(columns=columnsTitles)\n",
    "columnsTitles = ['English_City', 'Arabic_City','Date', 'Number of Transactions', 'Value of Transactions']\n",
    "cities_df = cities_df.reindex(columns=columnsTitles)\n",
    "\n",
    "sectors_df.to_csv('output/sectors_df.csv', index = False)\n",
    "# merge cities_df and Grouped_City in order to standerlize df, it we help us alot once we plot our data\n",
    "full_cities_df = pd.merge(cities_df, Grouped_City[['English_City', 'location_latitude', 'location_longitude']], on='English_City')\n",
    "full_cities_df.to_csv('output/full_cities_df.csv', index = False)\n",
    "\n",
    "print(\"** Done cleansing data frames **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Under improvment\n",
    "import openpyxl\n",
    "\n",
    "wb_obj = openpyxl.load_workbook(\"./input/Monthly_Bulletin_September2021.xlsx\") \n",
    "\n",
    "# Read the active sheet:\n",
    "sheet = wb_obj[\"30e\"]\n",
    "columnsTitles = ['English_City', 'Arabic_City','Date', 'Number of Transactions', 'Value of Transactions']\n",
    "cities_ar = ['تبوك','حائل','أبها','مكة المكرمة','بريدة','الخبر','المنورة','الدمام','جدة','الرياض','المدن الأخرى']\n",
    "cities_en = ['TABOUK','HAIL','ABHA','MAKKAH','BURAIDAH','KHOBAR','MADINA','DAMMAM','JEDDAH','RIYADH','OTHER']\n",
    "\n",
    "df = pd.DataFrame(sheet.values)\n",
    "df = df.iloc[13: , :]\n",
    "cols = [0,19,20,39,40,55,56,57]\n",
    "df.drop(df.columns[cols],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under improvment\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "df = pd.read_excel(\"./input/Monthly_Bulletin_September2021.xlsx\",\n",
    "                   sheet_name='30e', engine='openpyxl', skiprows=14)\n",
    "df.dropna(axis = 0, how = 'all', inplace = True)\n",
    "df.dropna(axis = 1, how = 'all', inplace = True)\n",
    "df.drop(df.index[[0]], inplace=True)\n",
    "df.drop(df.index[2:30], inplace=True)\n",
    "df.dropna(axis = 1, how = 'all', inplace = True)\n",
    "df.drop(df.columns[[16, 32]], axis=1, inplace=True)\n",
    "columns_name = {'الفترة': 'Date','Riyadh': 'Riyadh Number of Transactions','Unnamed: 4': 'Riyadh Value of Transactions','Unnamed: 5': 'Riyadh Number of Terminals','Jeddah': 'Jeddah Number of Transactions','Unnamed: 7': 'Jeddah Value of Transactions','Unnamed: 8': 'Jeddah Number of Terminals','Dammam': 'Dammam Number of Transactions','Unnamed: 10': 'Dammam Value of Transactions','Unnamed: 11': 'Dammam Number of Terminals','AL-Madinah': 'AL-Madinah Number of Transactions','Unnamed: 13': 'AL-Madinah Value of Transactions','Unnamed: 14': 'AL-Madinah Number of Terminals','Makkah': 'Makkah Number of Transactions','Unnamed: 16': 'Makkah Value of Transactions','Unnamed: 17': 'Makkah Number of Terminals','Buraidah': 'Buraidah Number of Transactions','Unnamed: 24': 'Buraidah Value of Transactions','Unnamed: 25': 'Buraidah Number of Terminals','Tabuk': 'Tabuk Number of Transactions','Unnamed: 27': 'Tabuk Value of Transactions','Unnamed: 28': 'Tabuk Number of Terminals','Hail': 'Hail Number of Transactions','Unnamed: 30': 'Hail Value of Transactions','Unnamed: 31': 'Hail Number of Terminals','Abha': 'Abha Number of Transactions','Unnamed: 33': 'Abha Value of Transactions','Unnamed: 34': 'Abha Number of Terminals','Jazan': 'Jazan Number of Transactions','Unnamed: 36': 'Jazan Value of Transactions','Unnamed: 37': 'Jazan Number of Terminals','Najran': 'Najran Number of Transactions','Unnamed: 44': 'Najran Value of Transactions','Unnamed: 45': 'Najran Number of Terminals','Skaka': 'Skaka Number of Transactions','Unnamed: 47': 'Skaka Value of Transactions','Unnamed: 48': 'Skaka Number of Terminals','Arar': 'Arar Number of Transactions','Unnamed: 50': 'Arar Value of Transactions','Unnamed: 51': 'Arar Number of Terminals','AL-Bahah': 'AL-Bahah Number of Transactions','Unnamed: 53': 'AL-Bahah Value of Transactions','Unnamed: 54': 'AL-Bahah Number of Terminals' }\n",
    "df.rename(columns=columns_name, inplace=True)\n",
    "df.drop(df.index[[0]], inplace=True)\n",
    "print(tabulate(df.head(35), headers='keys', tablefmt='psql'))\n",
    "df.to_csv(\"./output/Monthly_pos_by_citites.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
